{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10893915,"sourceType":"datasetVersion","datasetId":6770124}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Load Data**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset\nfile_path = r\"/kaggle/input/codedataset/spoc-train-train.tsv\"  # Change to your dataset's path\ndf = pd.read_csv(file_path, sep=\"\\t\")  # Specify tab separator\n\n# Display dataset information\nprint(df.info())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T05:54:33.270577Z","iopub.execute_input":"2025-03-02T05:54:33.271099Z","iopub.status.idle":"2025-03-02T05:54:33.626154Z","shell.execute_reply.started":"2025-03-02T05:54:33.271044Z","shell.execute_reply":"2025-03-02T05:54:33.625323Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 246086 entries, 0 to 246085\nData columns (total 7 columns):\n #   Column    Non-Null Count   Dtype \n---  ------    --------------   ----- \n 0   text      181862 non-null  object\n 1   code      246086 non-null  object\n 2   workerid  246086 non-null  int64 \n 3   probid    246086 non-null  object\n 4   subid     246086 non-null  int64 \n 5   line      246086 non-null  int64 \n 6   indent    246086 non-null  int64 \ndtypes: int64(4), object(3)\nmemory usage: 13.1+ MB\nNone\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"**Data Preprocessing**","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nimport nltk\n\n# Download the 'punkt' tokenizer data\nnltk.download('punkt')\n\n# If 'punkt_tab' is still missing, try:\nnltk.download('punkt_tab')\n\n\n# Download punkt tokenizer\nnltk.download('punkt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T05:54:36.861768Z","iopub.execute_input":"2025-03-02T05:54:36.862095Z","iopub.status.idle":"2025-03-02T05:54:36.869981Z","shell.execute_reply.started":"2025-03-02T05:54:36.862069Z","shell.execute_reply":"2025-03-02T05:54:36.869156Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"df[\"text\"] = df[\"text\"].astype(str)\ndf[\"text\"] = df[\"text\"].astype(str).fillna(\"\").replace(\"nan\", \"\")\n\ndf[\"text_tokens\"] = df[\"text\"].apply(word_tokenize)\nprint(df[\"text_tokens\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T05:54:39.558177Z","iopub.execute_input":"2025-03-02T05:54:39.558451Z","iopub.status.idle":"2025-03-02T05:54:53.906555Z","shell.execute_reply.started":"2025-03-02T05:54:39.558430Z","shell.execute_reply":"2025-03-02T05:54:53.905718Z"}},"outputs":[{"name":"stdout","text":"0                                                  []\n1                                 [create, string, s]\n2         [create, integers, x1, ,, y1, ,, x2, ,, y2]\n3                                           [read, s]\n4                    [set, x1, to, s, [, 0, ], -, 96]\n                             ...                     \n246081                                             []\n246082                                             []\n246083             [print, ``, YES, '', and, newline]\n246084                                             []\n246085                                             []\nName: text_tokens, Length: 246086, dtype: object\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"df[\"code\"] = df[\"code\"].astype(str).fillna(\"\").replace(\"nan\", \"\")\ndf[\"code_tokens\"] = df[\"code\"].apply(word_tokenize)\nprint(df[\"code_tokens\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T05:54:57.367944Z","iopub.execute_input":"2025-03-02T05:54:57.368279Z","iopub.status.idle":"2025-03-02T05:55:15.381556Z","shell.execute_reply.started":"2025-03-02T05:54:57.368254Z","shell.execute_reply":"2025-03-02T05:55:15.380760Z"}},"outputs":[{"name":"stdout","text":"0                             [int, main, (, ), {]\n1                                   [string, s, ;]\n2                [int, x1, ,, y1, ,, x2, ,, y2, ;]\n3                                [cin, >, >, s, ;]\n4                    [x1, =, s, [, 0, ], -, 96, ;]\n                            ...                   \n246081                                         [}]\n246082                                         [}]\n246083    [cout, <, <, ``, YES, '', <, <, endl, ;]\n246084                              [return, 0, ;]\n246085                                         [}]\nName: code_tokens, Length: 246086, dtype: object\n","output_type":"stream"}],"execution_count":60},{"cell_type":"markdown","source":"**Sample Output**","metadata":{}},{"cell_type":"code","source":"# Print samples from index 1-4\nprint(\"Samples from index 1-4:\")\nfor i in range(1, 4):\n    print(f\"Index {i}:\")\n    print(\"Tokenized Pseudocode:\", df[\"text_tokens\"].iloc[i])\n    print(\"Tokenized C++ Code:\", df[\"code_tokens\"].iloc[i])\n    print(\"-\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T05:55:20.809084Z","iopub.execute_input":"2025-03-02T05:55:20.809405Z","iopub.status.idle":"2025-03-02T05:55:20.818010Z","shell.execute_reply.started":"2025-03-02T05:55:20.809379Z","shell.execute_reply":"2025-03-02T05:55:20.817203Z"}},"outputs":[{"name":"stdout","text":"Samples from index 1-4:\nIndex 1:\nTokenized Pseudocode: ['create', 'string', 's']\nTokenized C++ Code: ['string', 's', ';']\n--------------------------------------------------\nIndex 2:\nTokenized Pseudocode: ['create', 'integers', 'x1', ',', 'y1', ',', 'x2', ',', 'y2']\nTokenized C++ Code: ['int', 'x1', ',', 'y1', ',', 'x2', ',', 'y2', ';']\n--------------------------------------------------\nIndex 3:\nTokenized Pseudocode: ['read', 's']\nTokenized C++ Code: ['cin', '>', '>', 's', ';']\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"# Save tokenized pseudocode and C++ code to CSV\noutput_file = \"/kaggle/working/tokenized_spoc.csv\"\ndf[[\"text_tokens\", \"code_tokens\"]].to_csv(output_file, index=False)\n\nprint(f\"Tokenized data saved to {output_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T05:55:24.413528Z","iopub.execute_input":"2025-03-02T05:55:24.413807Z","iopub.status.idle":"2025-03-02T05:55:25.339149Z","shell.execute_reply.started":"2025-03-02T05:55:24.413786Z","shell.execute_reply":"2025-03-02T05:55:25.338337Z"}},"outputs":[{"name":"stdout","text":"Tokenized data saved to /kaggle/working/tokenized_spoc.csv\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"# Add start and end tokens to tokenized C++ code\ndf[\"code_tokens\"] = df[\"code_tokens\"].apply(lambda tokens: [\"<start>\"] + tokens + [\"<end>\"])\n# Save updated tokenized data to CSV\noutput_file = \"tokenized_spoc_with_tokens.csv\"\ndf[[\"text_tokens\", \"code_tokens\"]].to_csv(output_file, index=False)\n\nprint(f\"Updated tokenized data saved to {output_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T05:55:27.841065Z","iopub.execute_input":"2025-03-02T05:55:27.841353Z","iopub.status.idle":"2025-03-02T05:55:29.562454Z","shell.execute_reply.started":"2025-03-02T05:55:27.841330Z","shell.execute_reply":"2025-03-02T05:55:29.561557Z"}},"outputs":[{"name":"stdout","text":"Updated tokenized data saved to tokenized_spoc_with_tokens.csv\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# for i in range(len(df)):  # Iterate over all rows\n#     text_tokens = df[\"text_tokens\"].iloc[i]  # Get text tokens\n#     code_tokens = df[\"code_tokens\"].iloc[i]  # Get code tokens\n    \n#     text_len = len(text_tokens)\n#     code_len = len(code_tokens)\n    \n#     max_len = max(text_len, code_len)  # Find the bigger length\n    \n#     # Pad the smaller list to match max_len\n#     if text_len < max_len:\n#         df.at[i, \"text_tokens\"] = text_tokens + [\"<pad>\"] * (max_len - text_len)\n    \n#     if code_len < max_len:\n#         df.at[i, \"code_tokens\"] = code_tokens + [\"<pad>\"] * (max_len - code_len)\n\n# Save padded tokenized data to CSV\noutput_file = \"tokenized-padded.csv\"\ndf[[\"text_tokens\", \"code_tokens\"]].to_csv(output_file, index=False)\n\nprint(f\"Padded tokenized data saved to {output_file}\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T06:05:01.471477Z","iopub.execute_input":"2025-03-02T06:05:01.471784Z","iopub.status.idle":"2025-03-02T06:05:02.787868Z","shell.execute_reply.started":"2025-03-02T06:05:01.471761Z","shell.execute_reply":"2025-03-02T06:05:02.787133Z"}},"outputs":[{"name":"stdout","text":"Padded tokenized data saved to tokenized-padded.csv\n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"import json\n\n# Define special tokens with fixed indices\nvocab = {\n    \"<unk>\": 0,\n    \"<pad>\": 1,\n    \"<start>\": 2,\n    \"<end>\": 3\n}\n\n# Assign indices to other tokens\nfor column in [\"text_tokens\", \"code_tokens\"]:\n    for tokens in df[column]:\n        for token in tokens:\n            if token not in vocab:\n                vocab[token] = len(vocab)\n\n# Save vocabulary to JSON\nvocab_file = \"vocabulary.json\"\nwith open(vocab_file, \"w\") as f:\n    json.dump(vocab, f, indent=4)\n\nprint(f\"Vocabulary saved to {vocab_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T06:06:26.771425Z","iopub.execute_input":"2025-03-02T06:06:26.771714Z","iopub.status.idle":"2025-03-02T06:06:27.465310Z","shell.execute_reply.started":"2025-03-02T06:06:26.771692Z","shell.execute_reply":"2025-03-02T06:06:27.464405Z"}},"outputs":[{"name":"stdout","text":"Vocabulary saved to vocabulary.json\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"# Load vocabulary\nwith open(\"/kaggle/working/vocabulary.json\", \"r\") as f:\n    vocab = json.load(f)\n\n# Load tokenized data\ndf = pd.read_csv(\"/kaggle/working/tokenized-padded.csv\")\n\n# Convert string tokens to lists\ndf[\"text_tokens\"] = df[\"text_tokens\"].apply(eval)\ndf[\"code_tokens\"] = df[\"code_tokens\"].apply(eval)\n\n# Convert tokens to sequences using vocabulary\ndf[\"text_sequences\"] = df[\"text_tokens\"].apply(lambda tokens: [vocab.get(token, vocab[\"<unk>\"]) for token in tokens])\ndf[\"code_sequences\"] = df[\"code_tokens\"].apply(lambda tokens: [vocab.get(token, vocab[\"<unk>\"]) for token in tokens])\n\n# Save sequences to CSV\noutput_file = \"tokenized_sequences.csv\"\ndf[[\"text_sequences\", \"code_sequences\"]].to_csv(output_file, index=False)\n\nprint(f\"Tokenized sequences saved to {output_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T06:08:52.600393Z","iopub.execute_input":"2025-03-02T06:08:52.600710Z","iopub.status.idle":"2025-03-02T06:09:04.440370Z","shell.execute_reply.started":"2025-03-02T06:08:52.600686Z","shell.execute_reply":"2025-03-02T06:09:04.439298Z"}},"outputs":[{"name":"stdout","text":"Tokenized sequences saved to tokenized_sequences.csv\n","output_type":"stream"}],"execution_count":78},{"cell_type":"markdown","source":"**Tensor DataType**","metadata":{"execution":{"iopub.status.busy":"2025-03-02T06:18:34.471076Z","iopub.execute_input":"2025-03-02T06:18:34.471360Z","iopub.status.idle":"2025-03-02T06:18:34.478338Z","shell.execute_reply.started":"2025-03-02T06:18:34.471337Z","shell.execute_reply":"2025-03-02T06:18:34.477615Z"}}},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nimport torch\nimport ast\nfrom torch.nn.utils.rnn import pad_sequence\nfrom tqdm import tqdm\n\nclass DataLoad(Dataset):\n    def __init__(self, file_path):\n        df = pd.read_csv(file_path)\n        self.inputs = [ast.literal_eval(x) for x in df['text_sequences']]\n        self.outputs = [ast.literal_eval(x) for x in df['code_sequences']]\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, idx):\n        input_tensor = torch.tensor(self.inputs[idx], dtype=torch.int64)\n        output_tensor = torch.tensor(self.outputs[idx], dtype=torch.int64)\n        return input_tensor, output_tensor\n\ndef Add_Pad(batch):\n    inputs, outputs = zip(*batch)\n    inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n    outputs = pad_sequence(outputs, batch_first=True, padding_value=0)\n    return inputs, outputs\n\n# Load dataset and dataloader\ndataset = DataLoad('/kaggle/working/tokenized_sequences.csv')\ndataloader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=Add_Pad)\n\n# Iterate with progress bar\ndata_iter = iter(dataloader)\nfor batch in tqdm(dataloader, desc=\"Loading Batches\"):\n    features, labels = batch  # Get a batch of data\n    break  # Remove this if you want to iterate over all batches\n\nprint(\"Batch loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T06:27:22.450987Z","iopub.execute_input":"2025-03-02T06:27:22.451307Z","iopub.status.idle":"2025-03-02T06:27:32.641985Z","shell.execute_reply.started":"2025-03-02T06:27:22.451282Z","shell.execute_reply":"2025-03-02T06:27:32.641231Z"}},"outputs":[{"name":"stderr","text":"Loading Batches:   0%|          | 0/3846 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Batch loaded successfully!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\n\n# Transformer Hyperparameters\nclass Config:\n    vocab_size =12388  # Adjust based on vocabulary.json\n    max_length = 100  # Adjust based on sequence length\n    embed_dim = 256\n    num_heads = 8\n    num_layers =2\n    feedforward_dim = 512\n    dropout = 0.1\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nconfig = Config()\n\n# Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self, embed_dim, max_len=100):\n        super(PositionalEncoding, self).__init__()\n        pe = torch.zeros(max_len, embed_dim)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-math.log(10000.0) / embed_dim))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.pe = pe.unsqueeze(0)  # Shape: (1, max_len, embed_dim)\n\n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)].to(x.device)\n\n# Transformer Model\nclass PseudoCodeTransformer(nn.Module):\n    def __init__(self, config):\n        super(PseudoCodeTransformer, self).__init__()\n        self.embedding = nn.Embedding(config.vocab_size, config.embed_dim)\n        self.positional_encoding = PositionalEncoding(config.embed_dim, config.max_length)\n\n        self.transformer = nn.Transformer(\n            d_model=config.embed_dim,\n            nhead=config.num_heads,\n            num_encoder_layers=config.num_layers,\n            num_decoder_layers=config.num_layers,\n            dim_feedforward=config.feedforward_dim,\n            dropout=config.dropout\n        )\n\n        self.fc_out = nn.Linear(config.embed_dim, config.vocab_size)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def generate_square_subsequent_mask(self, sz):\n        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(config.device)\n\n    def forward(self, src, tgt):\n        src_emb = self.embedding(src) * math.sqrt(config.embed_dim)\n        tgt_emb = self.embedding(tgt) * math.sqrt(config.embed_dim)\n\n        src_emb = self.positional_encoding(src_emb)\n        tgt_emb = self.positional_encoding(tgt_emb)\n\n        src_mask = self.generate_square_subsequent_mask(src.size(1))\n        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1))\n\n        out = self.transformer(src_emb.permute(1, 0, 2), tgt_emb.permute(1, 0, 2),\n                               src_mask=src_mask, tgt_mask=tgt_mask)\n\n        out = self.fc_out(out.permute(1, 0, 2))  # Convert back to batch-first\n        return out\n\n# Initialize Model\nmodel = PseudoCodeTransformer(config).to(config.device)\nprint(\"Model initialized successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T06:45:27.108480Z","iopub.execute_input":"2025-03-02T06:45:27.108769Z","iopub.status.idle":"2025-03-02T06:45:27.209626Z","shell.execute_reply.started":"2025-03-02T06:45:27.108749Z","shell.execute_reply":"2025-03-02T06:45:27.208882Z"}},"outputs":[{"name":"stdout","text":"Model initialized successfully!\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"def translate(model, pseudocode_tokens, vocab, device, max_length=50):\n    model.eval()\n\n    # Convert pseudocode tokens to numerical indices\n    input_ids = [vocab.get(token, vocab[\"<unk>\"]) for token in pseudocode_tokens]\n    input_tensor = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)  # Add batch dimension\n\n    # Start token for generation\n    output_ids = [vocab[\"<start>\"]]\n\n    for _ in range(max_length):\n        output_tensor = torch.tensor(output_ids, dtype=torch.long).unsqueeze(0).to(device)\n\n        # Get model predictions\n        with torch.no_grad():\n            predictions = model(input_tensor, output_tensor)\n\n        # Select the most probable token\n        next_token_id = predictions.argmax(dim=-1)[:, -1].item()\n        output_ids.append(next_token_id)\n\n        # Stop if end token is generated\n        if next_token_id == vocab[\"<end>\"]:\n            break\n\n    # Convert token indices back to words\n    id_to_token = {idx: token for token, idx in vocab.items()}\n    translated_code = [id_to_token.get(idx, \"<unk>\") for idx in output_ids[1:]]  # Exclude <start> token\n\n    return \" \".join(translated_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T06:45:31.489078Z","iopub.execute_input":"2025-03-02T06:45:31.489389Z","iopub.status.idle":"2025-03-02T06:45:31.495624Z","shell.execute_reply.started":"2025-03-02T06:45:31.489364Z","shell.execute_reply":"2025-03-02T06:45:31.494665Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"import json\n\n# Load vocabulary\nwith open(\"vocabulary.json\", \"r\") as f:\n    vocab = json.load(f)\n\n# Ensure vocab is a dictionary\nprint(f\"✅ Vocabulary loaded with {len(vocab)} tokens\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T06:45:34.504863Z","iopub.execute_input":"2025-03-02T06:45:34.505202Z","iopub.status.idle":"2025-03-02T06:45:34.516939Z","shell.execute_reply.started":"2025-03-02T06:45:34.505174Z","shell.execute_reply":"2025-03-02T06:45:34.516046Z"}},"outputs":[{"name":"stdout","text":"✅ Vocabulary loaded with 12388 tokens\n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport os\n\n# Check for GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"🔹 Using device: {device}\")\n\n# Move model to device\nmodel.to(device)\n\n# Loss Function & Optimizer\ncriterion = nn.CrossEntropyLoss(ignore_index=1)  # Ignore padding token\noptimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-3)\n\n# Create directory to save models\nos.makedirs(\"checkpoints\", exist_ok=True)\n\n# Training Loop\nnum_epochs = 1\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n\n    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n    for batch in progress_bar:\n        src, tgt = batch\n        src, tgt = src.to(device), tgt.to(device)  # Move batch to GPU\n\n        tgt_input = tgt[:, :-1]  # Remove <end> token\n        tgt_output = tgt[:, 1:]  # Shifted version\n\n        optimizer.zero_grad()\n        output = model(src, tgt_input)\n\n        loss = criterion(output.view(-1, config.vocab_size), tgt_output.contiguous().view(-1))\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        progress_bar.set_postfix(loss=loss.item())\n\n    avg_loss = epoch_loss / len(dataloader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n\n    # Save Model Checkpoint\n    torch.save(model.state_dict(), f\"checkpoints/transformer_epoch_{epoch+1}.pth\")\n    print(f\"✅ Model saved: checkpoints/transformer_epoch_{epoch+1}.pth\")\n\n    # Print Example Prediction\n    model.eval()\n    example_pseudocode = [\"create\", \"integer\", \"x\"]\n    translated_code = translate(model, example_pseudocode, vocab, device)\n    print(f\"🔹 Example Prediction (Pseudocode → C++): {translated_code}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T06:46:20.490573Z","iopub.execute_input":"2025-03-02T06:46:20.490895Z","iopub.status.idle":"2025-03-02T06:49:00.369953Z","shell.execute_reply.started":"2025-03-02T06:46:20.490868Z","shell.execute_reply":"2025-03-02T06:49:00.369203Z"}},"outputs":[{"name":"stdout","text":"🔹 Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/1: 100%|██████████| 3846/3846 [02:38<00:00, 24.33it/s, loss=0.105] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/1], Loss: 0.2908\n✅ Model saved: checkpoints/transformer_epoch_1.pth\n🔹 Example Prediction (Pseudocode → C++): int x , x ; <end>\n\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"example_pseudocode = [\"for\", \"i\", \"=\", \"0\", \"to\", \"size\", \"of\", \"ans\", \"exclusive\", \",\", \"print\", \"ans\", \"[\", \"i\", \"]\", \"print\", \"newline\"]\ntranslated_code = translate(model, example_pseudocode, vocab, device)\nprint(f\"🔹 Example Prediction (Pseudocode → C++): {translated_code}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T06:49:22.170922Z","iopub.execute_input":"2025-03-02T06:49:22.171444Z","iopub.status.idle":"2025-03-02T06:49:22.271609Z","shell.execute_reply.started":"2025-03-02T06:49:22.171417Z","shell.execute_reply":"2025-03-02T06:49:22.270865Z"}},"outputs":[{"name":"stdout","text":"🔹 Example Prediction (Pseudocode → C++): for ( int i = 0 ; i < ans ; i++ ) { cout < ans [ i ] ; } <end>\n\n","output_type":"stream"}],"execution_count":94},{"cell_type":"code","source":"# Load the trained model\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"🔹 Using device: {device}\")\n\n# Move model to device\nmodel.to(device)\n\nmodel = PseudoCodeTransformer(config).to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/working/checkpoints/transformer_epoch_1.pth\", map_location=device))\nmodel.eval()\n\n# Run translation on example pseudocode\nexample_pseudocode = [\"for\", \"i\", \"=\", \"0\", \"to\", \"size\", \"of\", \"ans\", \"exclusive\", \",\", \"print\", \"ans\", \"[\", \"i\", \"]\", \"print\", \"newline\"]\ntranslated_code = translate(model, example_pseudocode, vocab, device)\nprint(\"Pseudocode: \",example_pseudocode)\nprint(f\"C++: {translated_code}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-02T06:56:13.778311Z","iopub.execute_input":"2025-03-02T06:56:13.778639Z","iopub.status.idle":"2025-03-02T06:56:13.992653Z","shell.execute_reply.started":"2025-03-02T06:56:13.778611Z","shell.execute_reply":"2025-03-02T06:56:13.991799Z"}},"outputs":[{"name":"stdout","text":"🔹 Using device: cuda\nPseudocode:  ['for', 'i', '=', '0', 'to', 'size', 'of', 'ans', 'exclusive', ',', 'print', 'ans', '[', 'i', ']', 'print', 'newline']\nC++: for ( int i = 0 ; i < ans ; i++ ) { cout < ans [ i ] ; } <end>\n\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-99-4bce44630ec3>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/working/checkpoints/transformer_epoch_1.pth\", map_location=device))\n","output_type":"stream"}],"execution_count":99}]}